{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74eb8f43-d242-439b-985e-963fbe3c0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as skTfidf\n",
    "import cupy as cp           #use cupy array instead of numpy to speed up calculation by using GPU\n",
    "import cudf as cf\n",
    "from cuml.metrics.pairwise_distances import sparse_pairwise_distances\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as cuTfidf\n",
    "from cuml.metrics.pairwise_distances import pairwise_distances\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606663a1-fb8b-4fb6-aa3f-978429bf347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/test/Data/corpus.csv'\n",
    "def load_frame(path_to_df=path, encoding='utf-16', filter_value = 100000):\n",
    "    \n",
    "    df= pd.read_csv(path_to_df, encoding=encoding, index_col='id')\n",
    "    df.drop([df.columns[0]], inplace=True, axis=1)\n",
    "    df.drop_duplicates(subset=['text'],inplace=True)\n",
    "    \n",
    "    exam_type_distribution = df.groupby(['exam_type'])['exam_type'].count()\n",
    "    exam_type_distribution.sort_values(ascending=False)\n",
    "    \n",
    "    list_filtered = exam_type_distribution[exam_type_distribution > filter_value].index\n",
    "    df_filtered = df[df['exam_type'].isin(list_filtered)]\n",
    "    \n",
    "    print(f'original dataframe shape: {df.shape}')\n",
    "    print(f'df_filtered shape: {df_filtered.shape}')\n",
    "    \n",
    "    print(f'numbers of exam types before: {len(set(df[\"exam_type\"]))}')\n",
    "    print(f'types after filtering: {list_filtered}')\n",
    "    print(f'number of exam types after filtering: {len(set(df_filtered[\"exam_type\"]))}')\n",
    "\n",
    "    return df_filtered, list_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70af3ac-981d-42ca-a71e-abd9f9cd9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a load function to load each exam type\n",
    "def load_exam_type(df, exam_types):\n",
    "    for i in exam_types:\n",
    "        print(f'exam type: {i}')\n",
    "        dataframe = df[df['exam_type'] == i]    \n",
    "        print(f'number of documents: {dataframe.shape}')\n",
    "        yield dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9db39a-5204-4e8d-b586-59f415369c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(sparseMatrix, size = 5000):\n",
    "    for idx, item in enumerate(range(0, sparseMatrix.shape[0], size)):\n",
    "        batch_sparseMatrix = sparseMatrix[item:item+size,:]\n",
    "        print(batch_sparseMatrix.shape, item)\n",
    "        yield batch_sparseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ed5f7f8-51dd-4c43-9169-2135ee9f693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to list all entries with the its similarity inside a threshhold\n",
    "def get_matches(upperbound, lowerbound, batch_size=12000, filter=100000):\n",
    "    # create a dictionary to store the results\n",
    "    results_dict = {}\n",
    "\n",
    "    # start with loading the dataframe\n",
    "    df_filtered = load_frame(filter_value = filter)\n",
    "    df_by_type = load_exam_type(df_filtered[0], df_filtered[1])\n",
    "    \n",
    "    #tfidf_batch = cuTfidf().fit_transform(df_by_type)\n",
    "    \n",
    "    # loop over all exam types\n",
    "    for i in range(0,len(df_filtered[1])):\n",
    "        dataframe = next(df_by_type)\n",
    "        #print(f'exam type: {dataframe[\"exam_type\"].iloc[0]}')\n",
    "        #for idx, row in df_by_type.iterrows(): \n",
    "            #row ist ein text \n",
    "        for idx, batch in enumerate(range(0, len(dataframe), batch_size)):\n",
    "            batch_info = f'batch number {idx}, rows {batch}:{batch + batch_size}'\n",
    "            # print(batch_info)\n",
    "            # transform the batchch+batch_size].index[i],\n",
    "            # create a tfIdf vectorizer and fit and transform the documents\n",
    "            batch_dataframe = dataframe.iloc[batch:batch + batch_size]\n",
    "            df_indices = batch_dataframe.index.to_list()\n",
    "            tfidf_batch = cuTfidf().fit_transform(batch_dataframe['text'])\n",
    "            # print(f'tfidf shape:{tfidf_batch.shape}')\n",
    "\n",
    "            # calculate the euclidean distance\n",
    "            distances_batch = sparse_pairwise_distances(tfidf_batch, metric='euclidean')\n",
    "            print(f'distances shape: {distances_batch.shape}')\n",
    "\n",
    "            results = euclidean_distance(distances_batch, batch_dataframe, upperbound=upperbound, lowerbound=lowerbound, indices=df_indices)\n",
    "            if results:\n",
    "                results_dict.update(results)\n",
    "            break\n",
    "        break\n",
    "    return results_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c585ad4-b5de-44ec-b62d-6b85afe7dd55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to calculate the euclidean distance of a batch of documents \n",
    "# and return the neighbours based on the threshold\n",
    "def euclidean_distance(distance_batch, batch_dataframe, upperbound, lowerbound, indices):\n",
    "\n",
    "    results = {}\n",
    "    distance_batch = distance_batch[0:50] #for sampling/testing purposes\n",
    "    \n",
    "    for i, row in enumerate(distance_batch):\n",
    "\n",
    "        arg_sorted = cp.argsort(row)  # use np.argsort is as fast as cp.argsort, maybe better for memory\n",
    "        sorted_array = row[arg_sorted]\n",
    "        \n",
    "        candidates = get_candidates(sorted_array, arg_sorted, upper=upperbound)\n",
    "                \n",
    "        df_candidates = [indices[int(i)] for i in candidates[1:]]\n",
    "        original_index = indices[i]\n",
    "        \n",
    "        if df_candidates:\n",
    "            results[original_index] = df_candidates\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1df950a-bbbe-485f-a4d6-1fa04b80446a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_candidates(sorted_array, arg_sorted, upper):\n",
    "    candidates = []\n",
    "    for i, x in enumerate(sorted_array):\n",
    "        if x > upper:\n",
    "            break\n",
    "        else:\n",
    "            candidates.append(int(arg_sorted[i]))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b4e6da-bdda-48ad-a44f-824af88c5c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataframe shape: (2586631, 4)\n",
      "df_filtered shape: (1845401, 4)\n",
      "numbers of exam types before: 985\n",
      "types after filtering: Index(['ARCK', 'ARCW', 'ARKK', 'ARKW', 'ARRB1EBBEK', 'ARRBXEB', 'ARREXEBO9L',\n",
      "       'ARREXEBO9R', 'ARREXEBOEL', 'ARREXEBOER', 'ARREXEBOGL', 'ARREXEBOGR',\n",
      "       'ARREXEBOHL', 'ARREXEBOHR', 'ARREXEBOSL', 'ARREXEBOSR', 'ARREXEBUFL',\n",
      "       'ARREXEBUFR', 'ARREXEBUGL', 'ARREXEBUGR', 'ARREXEBUHL', 'ARREXEBUHR',\n",
      "       'ARREXEBUKL', 'ARREXEBUKR', 'ARRKOPG', 'ARRKXEBNNH', 'ARRT', 'ARRTRBS',\n",
      "       'ARRWXEBBWS', 'ARRWXEBHWS', 'ARRWXEBLWS', 'ARSB', 'ARSBSNONIX',\n",
      "       'ARSBSNOTXN', 'ARSEFKVUVX', 'ARSESNOUXX', 'ARSXSNOWGW', 'ARXXTLE'],\n",
      "      dtype='object', name='exam_type')\n",
      "number of exam types after filtering: 38\n",
      "exam type: ARCK\n",
      "number of documents: (100888, 4)\n",
      "distances shape: (12000, 12000)\n"
     ]
    }
   ],
   "source": [
    "test = get_matches(upperbound=0.2, lowerbound=0, batch_size=12000, filter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4950887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4abb12-9ddb-491a-aaef-22c28724d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b6fac-4f8e-4623-abfd-bec334f6def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = get_matches(upperbound=0.75, lowerbound=0, batch_size=30000, filter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab2e54-c882-44f1-bc9f-7398c3788b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bb46c-50e1-47b7-a606-e6649bf5a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_frame(filter_value=10000)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ee204-8949-4b82-a5e8-8116b659af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[130]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555609df-c5a7-47c0-88ab-c07d6eda19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[476196]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d7563-36f9-4183-b653-8b9f3a045830",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75c694-512b-4af8-b13d-b851c5dffd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_frame()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e28ae-19ab-4732-b8b2-a1da941a42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[2135242]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf285aab-3779-4147-a071-55c56575a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2147647]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfef05-b894-4d0b-96d7-67a44f49b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_items = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89975d01-378f-4bbb-982a-85ae67297823",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test.items():\n",
    "    print(f'i[0] {i[0]}')\n",
    "    print(f'i[1] {i[1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cec34d-a7e6-4ab7-b738-787734c72376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, j in test.items():\n",
    "    print(i)\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e6a7e8-4931-4c21-8c24-b7d472dd9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in test.items():\n",
    "    items_to_delete = [i for i in j if i not in deleted_items]\n",
    "    print(i, items_to_delete)\n",
    "    for item in items_to_delete:\n",
    "        deleted_items.append(item)\n",
    "        print(f'deleted item: {deleted_items}')\n",
    "    if i not in deleted_items:\n",
    "        print('True')\n",
    "        df = df.drop([*items_to_delete])\n",
    "        deleted_items.append(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rapids')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "53f3eceb40f4009eb0066205ad6cbfee3ad87f86a08e038ce869917275aafa93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
